{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcf1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras import regularizers\n",
    "from timeit import default_timer as timer\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a970cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check that GPU available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2656128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams problem\n",
    "number_of_different_chars=5\n",
    "sentence_length=5\n",
    "batch_size=64 #how many episodes for one training step (=parameter update)?\n",
    "\n",
    "epoch_size=1 #how many training steps to count as 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77a3c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'random_uniform:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_1:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_2:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_3:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_4:0' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "### create dataset: random strings \n",
    "\n",
    "#function to create one random string with chars encoded as one-hot vectors. \n",
    "#probably there is a more efficient way to do this?\n",
    "def create_random_string(number_of_different_chars, sentence_length):\n",
    "    #random_string=[tf.one_hot(tf.random.uniform(shape=(), minval=0, maxval=number_of_different_chars, dtype=tf.int32), number_of_different_chars) for i in range(sentence_length)]\n",
    "    random_string=[tf.random.uniform(shape=(), minval=0, maxval=number_of_different_chars, dtype=tf.int32) for i in range(sentence_length)]\n",
    "    print(random_string)\n",
    "    return tf.convert_to_tensor(random_string)\n",
    "\n",
    "# https://stackoverflow.com/questions/47318734/on-the-fly-generation-with-dataset-api-tensorflow\n",
    "dummy_dataset = tf.data.Dataset.from_tensors(0).repeat(batch_size * epoch_size)\n",
    "dataset = dummy_dataset.map(lambda _: create_random_string(number_of_different_chars, sentence_length))\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681a0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3 2 2 0 4]\n",
      " [3 4 2 1 4]\n",
      " [2 1 4 2 1]\n",
      " [3 4 2 4 3]\n",
      " [4 3 0 3 0]\n",
      " [3 2 0 1 3]\n",
      " [3 1 3 4 2]], shape=(7, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "### display some examples from the dataset to check\n",
    "i=0\n",
    "for element in dataset:\n",
    "    print(element[:7])   \n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a99e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d664ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams neural net\n",
    "#intermediate_dim=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adb543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 5, 5)              25        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 5, 5)              30        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### create the neural net (NN1) of agent1 \n",
    "\n",
    "NN1_input = keras.Input((sentence_length))\n",
    "e = keras.layers.Embedding(number_of_different_chars, number_of_different_chars)(NN1_input)\n",
    "NN1_output = keras.layers.Conv1D(number_of_different_chars, (1))(e) #returns the logits!! no softmax\n",
    "\n",
    "NN1 = keras.Model(NN1_input, NN1_output)\n",
    "NN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38788d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee58f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define reward-function, here: (sentence_length - HammingDistance)/sentence_length\n",
    "def compute_rewards(predicted_string, correct_string):\n",
    "    #check where predicted_string[i]=correct_string[i]\n",
    "    character_matches=tf.math.equal(predicted_string, correct_string)\n",
    "    #convert True, False to 1, 0\n",
    "    character_matches_as_ints=tf.cast(character_matches, tf.float32)\n",
    "    #sum to get the reward\n",
    "    reward=tf.math.reduce_sum(character_matches_as_ints, axis=-1)\n",
    "    #divide by sentence_length for correct scaling\n",
    "    return reward/sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3143a52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.4, 1. , 0.8], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test reward function\n",
    "a=tf.constant([[1, 0, 0, 1, 1], [1, 0, 0, 1, 1], [1, 0, 0, 1, 1]], dtype=tf.int32)\n",
    "b=tf.constant([[0, 1, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 0]], dtype=tf.int32)\n",
    "compute_rewards(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6329bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0) #, clipnorm=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4651f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(agent_output, probability_logits, rewards):\n",
    "    agent_output_one_hot=tf.one_hot(agent_output, number_of_different_chars)\n",
    "    log_lik = agent_output_one_hot*probability_logits\n",
    "    return tf.math.reduce_sum(-log_lik*rewards[:, np.newaxis, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9955551",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define a train step (for a single agent)\n",
    "def train_step(input_batch, NN1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        chararacter_probabilites_logits = NN1(input_batch, training=True) #\n",
    "        \n",
    "        ## sample from the probabilities to decide what action the agent chose\n",
    "        # https://stackoverflow.com/questions/39432164/sample-from-a-tensor-in-tensorflow-along-an-axis\n",
    "        dims = chararacter_probabilites_logits.get_shape().as_list() \n",
    "        #(dims = [batch_size, sentence_length, number_of_different_chars])\n",
    "        N = dims[-1]\n",
    "        logits = tf.reshape(chararacter_probabilites_logits, [-1, N])\n",
    "        samples = tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        agent1_output = tf.reshape(samples, dims[:-1])\n",
    "        \n",
    "\n",
    "        rewards = compute_rewards(agent1_output, input_batch)  \n",
    "        ## normalize rewards\n",
    "        mean = tf.math.reduce_mean(rewards)\n",
    "        std = tf.math.reduce_std(rewards) if tf.math.reduce_std(rewards) > 0 else 1.\n",
    "        scaled_rewards = (rewards-mean) / std\n",
    "        \n",
    "        loss = custom_loss(agent1_output, chararacter_probabilites_logits, scaled_rewards)\n",
    "\n",
    "    #retrieve gradients\n",
    "    grads = tape.gradient(loss, NN1.trainable_weights)\n",
    "    \n",
    "    #perform a parameter update\n",
    "    optimizer.apply_gradients(zip(grads, NN1.trainable_weights))\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aff589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea7725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training reward (for one batch):  0.15937501\n",
      "\n",
      "Start of epoch 1\n",
      "Training reward (for one batch):  0.22500002\n",
      "\n",
      "Start of epoch 2\n",
      "Training reward (for one batch):  0.23437501\n",
      "\n",
      "Start of epoch 3\n",
      "Training reward (for one batch):  0.33125\n",
      "\n",
      "Start of epoch 4\n",
      "Training reward (for one batch):  0.37187505\n",
      "\n",
      "Start of epoch 5\n",
      "Training reward (for one batch):  0.421875\n",
      "\n",
      "Start of epoch 6\n",
      "Training reward (for one batch):  0.47812504\n",
      "\n",
      "Start of epoch 7\n",
      "Training reward (for one batch):  0.665625\n",
      "\n",
      "Start of epoch 8\n",
      "Training reward (for one batch):  0.75\n",
      "\n",
      "Start of epoch 9\n",
      "Training reward (for one batch):  0.8156251\n"
     ]
    }
   ],
   "source": [
    "### Training loops\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    loss_count, epoch_average = 0, 0\n",
    "    \n",
    "    for step, input_batch in enumerate(dataset):\n",
    "        rewards = train_step(input_batch, NN1)\n",
    "        print(\"Training reward (for one batch): \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855aa9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91528c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_input_and_output(input_batch, neural_net, number_of_examples):\n",
    "    for i in range(number_of_examples):\n",
    "        print(list(input_batch[i].numpy()), \"\\t\", np.argmax(neural_net(np.array([input_batch[i]])), axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80150e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 2, 4] \t [1 4 4 2 4]\n",
      "[0, 2, 4, 1, 1] \t [0 2 4 1 1]\n",
      "[0, 2, 1, 4, 4] \t [0 2 1 4 4]\n",
      "[4, 1, 0, 3, 0] \t [4 1 0 3 0]\n",
      "[1, 0, 0, 0, 3] \t [1 0 0 0 3]\n"
     ]
    }
   ],
   "source": [
    "compare_input_and_output(input_batch, NN1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17659799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b16d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average over n samples: how many epochs did it take to reach a reward >= threshold once?\n",
    "def estimate_average_solving_time(n, threshold=1.):\n",
    "    sum_of_epochs=0\n",
    "    for _ in range(n):\n",
    "        ## create a new neural network\n",
    "        NN_input = keras.Input((sentence_length))\n",
    "        e = keras.layers.Embedding(number_of_different_chars, number_of_different_chars)(NN_input)\n",
    "        NN_output = keras.layers.Conv1D(number_of_different_chars, (1))(e) #returns the logits!! no softmax\n",
    "        NN = keras.Model(NN_input, NN_output)\n",
    "\n",
    "        ## train neural network and stop when reward >= threshold\n",
    "        epoch = 0\n",
    "        reward = 0\n",
    "        while reward < threshold:    \n",
    "            for step, input_batch in enumerate(dataset):\n",
    "                rewards = train_step(input_batch, NN)\n",
    "                reward = np.mean(rewards)       \n",
    "            epoch+=1\n",
    "\n",
    "        sum_of_epochs+=epoch\n",
    "        print(epoch)    \n",
    "    return sum_of_epochs / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8032853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "26\n",
      "18\n",
      "26\n",
      "21\n",
      "19\n",
      "27\n",
      "22\n",
      "24\n",
      "20\n",
      "18\n",
      "17\n",
      "51\n",
      "24\n",
      "18\n",
      "21\n",
      "20\n",
      "21\n",
      "22\n",
      "22\n",
      "21\n",
      "21\n",
      "25\n",
      "17\n",
      "22\n",
      "21\n",
      "27\n",
      "26\n",
      "21\n",
      "24\n",
      "24\n",
      "28\n",
      "22\n",
      "23\n",
      "21\n",
      "25\n",
      "24\n",
      "30\n",
      "25\n",
      "18\n",
      "17\n",
      "21\n",
      "20\n",
      "21\n",
      "23\n",
      "18\n",
      "20\n",
      "26\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.62"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_average_solving_time(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a409f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
