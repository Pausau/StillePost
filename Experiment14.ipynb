{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcf1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras import regularizers\n",
    "from timeit import default_timer as timer\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a970cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check that GPU available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2656128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams problem\n",
    "number_of_different_chars=3\n",
    "number_of_different_intermediate_chars=10 # (!!)\n",
    "sentence_length=5\n",
    "batch_size=64 #how many episodes for one training step (=parameter update)?\n",
    "\n",
    "epoch_size=1 #how many training steps to count as 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77a3c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'random_uniform:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_1:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_2:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_3:0' shape=() dtype=int32>, <tf.Tensor 'random_uniform_4:0' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "### create dataset: random strings \n",
    "\n",
    "#function to create one random string with chars encoded as one-hot vectors. \n",
    "#probably there is a more efficient way to do this?\n",
    "def create_random_string(number_of_different_chars, sentence_length):\n",
    "    #random_string=[tf.one_hot(tf.random.uniform(shape=(), minval=0, maxval=number_of_different_chars, dtype=tf.int32), number_of_different_chars) for i in range(sentence_length)]\n",
    "    random_string=[tf.random.uniform(shape=(), minval=0, maxval=number_of_different_chars, dtype=tf.int32) for i in range(sentence_length)]\n",
    "    print(random_string)\n",
    "    return tf.convert_to_tensor(random_string)\n",
    "\n",
    "# https://stackoverflow.com/questions/47318734/on-the-fly-generation-with-dataset-api-tensorflow\n",
    "dummy_dataset = tf.data.Dataset.from_tensors(0).repeat(batch_size * epoch_size)\n",
    "dataset = dummy_dataset.map(lambda _: create_random_string(number_of_different_chars, sentence_length))\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681a0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 0 1 1 1]\n",
      " [2 2 2 0 1]\n",
      " [0 2 2 0 1]\n",
      " [0 2 2 1 0]\n",
      " [0 0 2 0 0]\n",
      " [2 1 0 0 2]\n",
      " [0 0 0 2 0]], shape=(7, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "### display some examples from the dataset to check\n",
    "i=0\n",
    "for element in dataset:\n",
    "    print(element[:7])   \n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a99e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d664ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams neural net\n",
    "intermediate_dim=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adb543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 5, 256)            768       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 5, 10)             2570      \n",
      "=================================================================\n",
      "Total params: 3,338\n",
      "Trainable params: 3,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### create the neural net (NN1) of agent1 \n",
    "\n",
    "NN1_input = keras.Input((sentence_length))\n",
    "e = keras.layers.Embedding(number_of_different_chars, intermediate_dim)(NN1_input)\n",
    "#e = keras.layers.Conv1D(intermediate_dim, (1))(e)\n",
    "NN1_output = keras.layers.Conv1D(number_of_different_intermediate_chars, (1))(e) #returns the logits!! no softmax\n",
    "\n",
    "NN1 = keras.Model(NN1_input, NN1_output)\n",
    "NN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5489ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 5, 256)            2560      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5, 3)              771       \n",
      "=================================================================\n",
      "Total params: 3,331\n",
      "Trainable params: 3,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### create the neural net (NN1) of agent1 \n",
    "\n",
    "NN2_input = keras.Input((sentence_length))\n",
    "e = keras.layers.Embedding(number_of_different_intermediate_chars, intermediate_dim)(NN2_input)\n",
    "#e = keras.layers.Conv1D(intermediate_dim, (1))(e)\n",
    "NN2_output = keras.layers.Conv1D(number_of_different_chars, (1))(e) #returns the logits!! no softmax\n",
    "\n",
    "NN2 = keras.Model(NN2_input, NN2_output)\n",
    "NN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38788d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee58f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define reward-function, here: (sentence_length - HammingDistance)/sentence_length\n",
    "def compute_rewards(predicted_string, correct_string):\n",
    "    #check where predicted_string[i]=correct_string[i]\n",
    "    character_matches=tf.math.equal(predicted_string, correct_string)\n",
    "    #convert True, False to 1, 0\n",
    "    character_matches_as_ints=tf.cast(character_matches, tf.float32)\n",
    "    #sum to get the reward\n",
    "    reward=tf.math.reduce_sum(character_matches_as_ints, axis=-1)\n",
    "    #divide by sentence_length for correct scaling\n",
    "    return reward/sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3143a52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.4, 1. , 0.8], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test reward function\n",
    "a=tf.constant([[1, 0, 0, 1, 1], [1, 0, 0, 1, 1], [1, 0, 0, 1, 1]], dtype=tf.int32)\n",
    "b=tf.constant([[0, 1, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 0]], dtype=tf.int32)\n",
    "compute_rewards(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6329bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0) #, clipnorm=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4651f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_intermediate(agent_output, probability_logits, rewards):\n",
    "    agent_output_one_hot=tf.one_hot(agent_output, number_of_different_intermediate_chars)\n",
    "    log_lik = agent_output_one_hot*probability_logits\n",
    "    return tf.math.reduce_sum(-log_lik*rewards[:, np.newaxis, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "431da8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_final(agent_output, probability_logits, rewards):\n",
    "    agent_output_one_hot=tf.one_hot(agent_output, number_of_different_chars)\n",
    "    log_lik = agent_output_one_hot*probability_logits\n",
    "    return tf.math.reduce_sum(-log_lik*rewards[:, np.newaxis, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9955551",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define a train step (for two agents)\n",
    "def train_step(input_batch, NN1, NN2):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \n",
    "        ## Agent 1\n",
    "        chararacter_probabilites_logits1 = NN1(input_batch, training=True) #\n",
    "        dims = chararacter_probabilites_logits1.get_shape().as_list() \n",
    "        N = dims[-1]\n",
    "        logits = tf.reshape(chararacter_probabilites_logits1, [-1, N])\n",
    "        samples = tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        agent1_output = tf.reshape(samples, dims[:-1])\n",
    "        \n",
    "        ## Agent 2\n",
    "        chararacter_probabilites_logits2 = NN2(agent1_output, training=True) #\n",
    "        dims = chararacter_probabilites_logits2.get_shape().as_list() \n",
    "        N = dims[-1]\n",
    "        logits = tf.reshape(chararacter_probabilites_logits2, [-1, N])\n",
    "        samples = tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        agent2_output = tf.reshape(samples, dims[:-1])\n",
    "        \n",
    "        ## Compute rewards\n",
    "        rewards = compute_rewards(agent2_output, input_batch)  \n",
    "        ## normalize rewards\n",
    "        mean = tf.math.reduce_mean(rewards)\n",
    "        std = tf.math.reduce_std(rewards) if tf.math.reduce_std(rewards) > 0 else 1.\n",
    "        scaled_rewards = (rewards-mean) / std\n",
    "        \n",
    "        loss1 = custom_loss_intermediate(agent1_output, chararacter_probabilites_logits1, scaled_rewards)\n",
    "        loss2 = custom_loss_final(agent2_output, chararacter_probabilites_logits2, scaled_rewards)\n",
    "\n",
    "    #retrieve gradients\n",
    "    grads1 = tape.gradient(loss1, NN1.trainable_weights)\n",
    "    grads2 = tape.gradient(loss2, NN2.trainable_weights)\n",
    "    del tape\n",
    "    \n",
    "    #perform a parameter update\n",
    "    optimizer.apply_gradients(zip(grads1, NN1.trainable_weights))\n",
    "    optimizer.apply_gradients(zip(grads2, NN2.trainable_weights))\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aff589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea7725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training reward (for one batch):  0.34062505\n",
      "\n",
      "Start of epoch 1\n",
      "Training reward (for one batch):  0.409375\n",
      "\n",
      "Start of epoch 2\n",
      "Training reward (for one batch):  0.346875\n",
      "\n",
      "Start of epoch 3\n",
      "Training reward (for one batch):  0.35625\n",
      "\n",
      "Start of epoch 4\n",
      "Training reward (for one batch):  0.296875\n",
      "\n",
      "Start of epoch 5\n",
      "Training reward (for one batch):  0.309375\n",
      "\n",
      "Start of epoch 6\n",
      "Training reward (for one batch):  0.33437502\n",
      "\n",
      "Start of epoch 7\n",
      "Training reward (for one batch):  0.33437502\n",
      "\n",
      "Start of epoch 8\n",
      "Training reward (for one batch):  0.35312504\n",
      "\n",
      "Start of epoch 9\n",
      "Training reward (for one batch):  0.37500003\n"
     ]
    }
   ],
   "source": [
    "### Training loops\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    loss_count, epoch_average = 0, 0\n",
    "    \n",
    "    for step, input_batch in enumerate(dataset):\n",
    "        rewards = train_step(input_batch, NN1, NN2)\n",
    "        print(\"Training reward (for one batch): \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855aa9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91528c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_input_and_output(input_batch, neural_net, number_of_examples):\n",
    "    for i in range(number_of_examples):\n",
    "        print(list(input_batch[i].numpy()), \"\\t\", np.argmax(neural_net(np.array([input_batch[i]])), axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80150e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 1] \t [2 2 2 2 2]\n",
      "[1, 2, 0, 2, 0] \t [2 2 2 2 2]\n",
      "[1, 1, 2, 0, 2] \t [2 2 2 2 2]\n",
      "[1, 0, 1, 1, 1] \t [2 2 2 2 2]\n",
      "[1, 2, 0, 0, 0] \t [2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "compare_input_and_output(input_batch, NN1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f389c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 1] \t [0 0 0 0 0]\n",
      "[1, 2, 0, 2, 0] \t [0 0 0 0 0]\n",
      "[1, 1, 2, 0, 2] \t [0 0 0 0 0]\n",
      "[1, 0, 1, 1, 1] \t [0 0 0 0 0]\n",
      "[1, 2, 0, 0, 0] \t [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "compare_input_and_output(input_batch, NN2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17659799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b16d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average over n samples: how many epochs did it take to reach a reward >= threshold once?\n",
    "def estimate_average_solving_time(n, max_epochs, threshold=1.):\n",
    "    sum_of_epochs=0\n",
    "    number_of_max_epochs_reached=0\n",
    "    for _ in range(n):\n",
    "        ## create NN1\n",
    "        NN1_input = keras.Input((sentence_length))\n",
    "        e = keras.layers.Embedding(number_of_different_chars, intermediate_dim)(NN1_input)\n",
    "        #e = keras.layers.Conv1D(intermediate_dim, (1))(e)\n",
    "        NN1_output = keras.layers.Conv1D(number_of_different_intermediate_chars, (1))(e) #returns the logits!! no softmax\n",
    "        NN1 = keras.Model(NN1_input, NN1_output)\n",
    "        \n",
    "        ## create NN2\n",
    "        NN2_input = keras.Input((sentence_length))\n",
    "        e = keras.layers.Embedding(number_of_different_intermediate_chars, intermediate_dim)(NN2_input)\n",
    "        #e = keras.layers.Conv1D(intermediate_dim, (1))(e)\n",
    "        NN2_output = keras.layers.Conv1D(number_of_different_chars, (1))(e) #returns the logits!! no softmax\n",
    "        NN2 = keras.Model(NN2_input, NN2_output)\n",
    "\n",
    "        ## train neural networks and stop when reward >= threshold\n",
    "        epoch = 0\n",
    "        reward = 0\n",
    "        while reward < threshold:    \n",
    "            for step, input_batch in enumerate(dataset):\n",
    "                rewards = train_step(input_batch, NN1, NN2)\n",
    "                reward = np.mean(rewards)       \n",
    "            epoch+=1\n",
    "            \n",
    "            if epoch>max_epochs:\n",
    "                print(\"Max epoch reached\")\n",
    "                number_of_max_epochs_reached+=1\n",
    "                break\n",
    "            \n",
    "        if not epoch>max_epochs:\n",
    "            sum_of_epochs+=epoch\n",
    "            print(epoch) \n",
    "        \n",
    "    #return average #parameter updates, percentage of not solved\n",
    "    return sum_of_epochs / (n-number_of_max_epochs_reached), number_of_max_epochs_reached/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8032853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "32\n",
      "28\n",
      "29\n",
      "34\n",
      "30\n",
      "31\n",
      "30\n",
      "27\n",
      "39\n",
      "32\n",
      "31\n",
      "31\n",
      "36\n",
      "34\n",
      "33\n",
      "64\n",
      "31\n",
      "31\n",
      "43\n",
      "42\n",
      "37\n",
      "28\n",
      "26\n",
      "32\n",
      "31\n",
      "32\n",
      "29\n",
      "28\n",
      "33\n",
      "34\n",
      "27\n",
      "26\n",
      "30\n",
      "Max epoch reached\n",
      "33\n",
      "Max epoch reached\n",
      "28\n",
      "30\n",
      "29\n",
      "30\n",
      "32\n",
      "24\n",
      "35\n",
      "29\n",
      "29\n",
      "29\n",
      "39\n",
      "39\n",
      "35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.625, 0.04)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_average_solving_time(50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a409f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
